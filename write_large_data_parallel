import multiprocessing

def generate_large_data_chunk(start, end):
    # Generate or fetch large data chunk (e.g., from a database)
    data_chunk = []
    for i in range(start, end):
        data_chunk.append(f"Line {i}\n")
    return data_chunk

def write_data_chunk_to_file(filename, data_chunk):
    # Open file in append mode ('a') to write chunk of data
    with open(filename, 'a') as f:
        for line in data_chunk:
            f.write(line)

def write_large_data_parallel(filename, total_lines, chunk_size=10000, num_processes=None):
    if num_processes is None:
        num_processes = multiprocessing.cpu_count()

    # Calculate number of chunks and distribute work among processes
    chunks = []
    for i in range(0, total_lines, chunk_size):
        chunks.append((i, min(i + chunk_size, total_lines)))

    # Create a pool of processes
    with multiprocessing.Pool(processes=num_processes) as pool:
        # Map the write function to chunks of data
        pool.starmap(write_data_chunk_to_file, [(filename, generate_large_data_chunk(start, end)) for start, end in chunks])

if __name__ == '__main__':
    filename = 'large_data_parallel.txt'
    total_lines = 1000000  # Total number of lines to generate and write
    chunk_size = 10000     # Number of lines per chunk
    num_processes = 4      # Number of processes to use

    write_large_data_parallel(filename, total_lines, chunk_size, num_processes)
    print(f"Large data has been written to {filename} using {num_processes} processes.")
